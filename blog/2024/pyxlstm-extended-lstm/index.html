<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Implementing Extended LSTM (xLSTM) - A Modern Take on Recurrent Networks | Mudit Bhargava </title> <meta name="author" content="Mudit Bhargava"> <meta name="description" content="Exploring PyxLSTM, an implementation of the Extended Long Short-Term Memory architecture with exponential gating and matrix memory"> <meta name="keywords" content="Signal processing, financial modeling, Machine Learning/AI, FPGA Development, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%99%8B%F0%9F%8F%BB%E2%80%8D%E2%99%82%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://muditbhargava66.github.io/blog/2024/pyxlstm-extended-lstm/"> <script src="/assets/js/theme.js?a5ca4084d3b81624bcfa01156dae2b8e"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Mudit</span> Bhargava </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/index.html">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/books/">bookshelf </a> </li> <li class="nav-item "> <a class="nav-link" href="/my-notes/">notes </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">more </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/people/">people</a> <a class="dropdown-item " href="/photography/">photography</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/teaching/">teaching</a> </div> </li> <li class="nav-item"> <button id="search-toggle" class="nav-link search-btn" title="Search (⌘K)"> <i class="fa-solid fa-magnifying-glass"></i> <span class="search-shortcut">⌘K</span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div id="search-modal" class="search-modal" style="display: none;"> <div class="search-modal-backdrop" onclick="closeSearchModal()"></div> <div class="search-modal-content"> <div class="search-modal-header"> <div class="search-input-wrapper"> <i class="fa-solid fa-magnifying-glass search-input-icon"></i> <input type="text" id="search-input" placeholder="Search pages, posts, projects..." autocomplete="off"> <span class="search-shortcut-hint">ESC</span> </div> </div> <div class="search-modal-body" id="search-results"> <div class="search-hint"> <p>Type to search across all content...</p> <div class="search-shortcuts-help"> <span><kbd>↑</kbd><kbd>↓</kbd> to navigate</span> <span><kbd>Enter</kbd> to select</span> <span><kbd>ESC</kbd> to close</span> </div> </div> </div> </div> </div> <script>!function(){function e(){s.style.display="flex",a.value="",a.focus(),document.body.style.overflow="hidden"}async function t(){try{const e=await fetch("/assets/js/search-data.json");e.ok&&(r=await e.json())}catch(e){r=[{title:"Home",url:"/",type:"page"},{title:"Publications",url:"/publications/",type:"page"},{title:"Projects",url:"/projects/",type:"page"},{title:"Blog",url:"/blog/",type:"page"},{title:"CV",url:"/cv/",type:"page"},{title:"Repositories",url:"/repositories/",type:"page"},{title:"Teaching",url:"/teaching/",type:"page"}]}}function n(e){if(!e.trim())return void(l.innerHTML='\n        <div class="search-hint">\n          <p>Type to search across all content...</p>\n          <div class="search-shortcuts-help">\n            <span><kbd>\u2191</kbd><kbd>\u2193</kbd> to navigate</span>\n            <span><kbd>Enter</kbd> to select</span>\n            <span><kbd>ESC</kbd> to close</span>\n          </div>\n        </div>\n      ');const t=r.filter(t=>t.title.toLowerCase().includes(e.toLowerCase())||t.content&&t.content.toLowerCase().includes(e.toLowerCase())).slice(0,8);0!==t.length?l.innerHTML=t.map((e,t)=>`\n      <a href="${e.url}" class="search-result-item ${0===t?"active":""}">\n        <span class="search-result-type">${e.type||"page"}</span>\n        <span class="search-result-title">${e.title}</span>\n      </a>\n    `).join(""):l.innerHTML='<div class="search-no-results">No results found</div>'}const s=document.getElementById("search-modal"),a=document.getElementById("search-input"),l=document.getElementById("search-results"),o=document.getElementById("search-toggle");let r=[];window.closeSearchModal=function(){s.style.display="none",document.body.style.overflow=""},document.addEventListener("keydown",function(t){(t.metaKey||t.ctrlKey)&&"k"===t.key&&(t.preventDefault(),"none"===s.style.display||""===s.style.display?e():closeSearchModal()),"Escape"===t.key&&"flex"===s.style.display&&closeSearchModal()}),o&&o.addEventListener("click",e),a&&(a.addEventListener("input",function(e){n(e.target.value)}),a.addEventListener("keydown",function(e){const t=l.querySelectorAll(".search-result-item"),n=l.querySelector(".search-result-item.active");if("ArrowDown"===e.key&&t.length>0){e.preventDefault();let s=0;n&&(n.classList.remove("active"),s=(Array.from(t).indexOf(n)+1)%t.length),t[s].classList.add("active")}else if("ArrowUp"===e.key&&t.length>0){e.preventDefault();let s=t.length-1;n&&(n.classList.remove("active"),s=(Array.from(t).indexOf(n)-1+t.length)%t.length),t[s].classList.add("active")}else"Enter"===e.key&&n&&(e.preventDefault(),window.location.href=n.href)})),t()}();</script> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Implementing Extended LSTM (xLSTM) - A Modern Take on Recurrent Networks</h1> <p class="post-meta"> Created in December 24, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/deep-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> deep-learning</a>   <a href="/blog/tag/lstm"> <i class="fa-solid fa-hashtag fa-sm"></i> lstm</a>   <a href="/blog/tag/pytorch"> <i class="fa-solid fa-hashtag fa-sm"></i> pytorch</a>   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp</a>     ·   <a href="/blog/category/projects"> <i class="fa-solid fa-tag fa-sm"></i> projects</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>The transformer architecture has dominated NLP for years, but recent research shows that recurrent networks aren’t finished evolving. Extended Long Short-Term Memory (xLSTM), introduced by Beck et al. in 2024, addresses fundamental limitations of traditional LSTMs while remaining competitive with transformers for certain tasks.</p> <p>This post explores <a href="https://github.com/muditbhargava66/PyxLSTM" rel="external nofollow noopener" target="_blank">PyxLSTM</a>, my implementation of the xLSTM architecture in PyTorch.</p> <h2 id="why-revisit-lstms">Why Revisit LSTMs?</h2> <p>Traditional LSTMs suffer from two key limitations:</p> <ol> <li> <strong>Limited storage capacity</strong> - The hidden state is a vector, constraining how much information can be retained</li> <li> <strong>Saturating gates</strong> - Sigmoid activations in gates compress gradients, making it hard to learn long-range dependencies</li> </ol> <p>xLSTM addresses both issues with two innovations: <strong>exponential gating</strong> and <strong>matrix memory</strong>.</p> <h2 id="the-xlstm-architecture">The xLSTM Architecture</h2> <h3 id="slstm-scalar-lstm-with-exponential-gating">sLSTM: Scalar LSTM with Exponential Gating</h3> <p>The scalar variant (sLSTM) replaces sigmoid gates with exponential activations:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Exponential Gate:  exp(g) instead of sigmoid(g)
</code></pre></div></div> <p>This seemingly simple change has profound effects:</p> <ul> <li> <strong>Eliminates saturation</strong> - Gradients flow more freely over long sequences</li> <li> <strong>Sharper gating</strong> - The model can make more decisive keep/forget decisions</li> <li> <strong>Better credit assignment</strong> - Information from distant timesteps is preserved</li> </ul> <h3 id="mlstm-matrix-memory">mLSTM: Matrix Memory</h3> <p>The matrix variant (mLSTM) replaces the hidden state vector with a full matrix:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Traditional LSTM: hidden state is vector
</span><span class="n">h</span> <span class="err">∈</span> <span class="n">ℝ</span><span class="o">^</span><span class="n">d</span>

<span class="c1"># mLSTM: hidden state is matrix
</span><span class="n">C</span> <span class="err">∈</span> <span class="n">ℝ</span><span class="o">^</span><span class="p">(</span><span class="n">d</span><span class="err">×</span><span class="n">d</span><span class="p">)</span>
</code></pre></div></div> <p>This square matrix provides d² storage capacity instead of just d, enabling the model to maintain richer representations of past information.</p> <h2 id="implementation-highlights">Implementation Highlights</h2> <h3 id="creating-an-xlstm-model">Creating an xLSTM Model</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">from</span> <span class="n">xLSTM.model</span> <span class="kn">import</span> <span class="n">xLSTM</span>

<span class="c1"># Initialize model
</span><span class="n">model</span> <span class="o">=</span> <span class="nf">xLSTM</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="mi">50000</span><span class="p">,</span>
    <span class="n">embedding_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">num_blocks</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
    <span class="n">lstm_type</span><span class="o">=</span><span class="sh">'</span><span class="s">mLSTM</span><span class="sh">'</span>  <span class="c1"># or 'sLSTM'
</span><span class="p">)</span>

<span class="c1"># Forward pass
</span><span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50000</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">))</span>
<span class="n">output</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>
</code></pre></div></div> <h3 id="block-structure-options">Block Structure Options</h3> <p>PyxLSTM supports two block configurations:</p> <p><strong>Pre-projection blocks</strong> - Project input up, apply LSTM, project down:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input → Linear(up) → LayerNorm → LSTM → Linear(down) → Output
</code></pre></div></div> <p><strong>Post-projection blocks</strong> - Apply LSTM first, then project:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Input → LSTM → LayerNorm → Linear(up) → GELU → Linear(down) → Output
</code></pre></div></div> <p>The choice depends on your compute budget and task requirements.</p> <h2 id="training-example">Training Example</h2> <p>Here’s how to train an xLSTM language model:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">xLSTM.data</span> <span class="kn">import</span> <span class="n">LanguageModelingDataset</span><span class="p">,</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="n">xLSTM.training</span> <span class="kn">import</span> <span class="n">train</span>
<span class="kn">from</span> <span class="n">xLSTM.utils</span> <span class="kn">import</span> <span class="n">load_config</span><span class="p">,</span> <span class="n">set_seed</span><span class="p">,</span> <span class="n">get_device</span>

<span class="c1"># Configuration
</span><span class="n">config</span> <span class="o">=</span> <span class="nf">load_config</span><span class="p">(</span><span class="sh">"</span><span class="s">configs/language_model.yaml</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">set_seed</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>
<span class="n">device</span> <span class="o">=</span> <span class="nf">get_device</span><span class="p">()</span>

<span class="c1"># Data
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">vocab_file</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="nc">LanguageModelingDataset</span><span class="p">(</span>
    <span class="n">config</span><span class="p">.</span><span class="n">train_data</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">config</span><span class="p">.</span><span class="n">max_length</span>
<span class="p">)</span>

<span class="c1"># Model
</span><span class="n">model</span> <span class="o">=</span> <span class="nf">xLSTM</span><span class="p">(</span>
    <span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">),</span>
    <span class="n">config</span><span class="p">.</span><span class="n">embedding_size</span><span class="p">,</span>
    <span class="n">config</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span>
    <span class="n">config</span><span class="p">.</span><span class="n">num_layers</span><span class="p">,</span>
    <span class="n">config</span><span class="p">.</span><span class="n">num_blocks</span><span class="p">,</span>
    <span class="n">config</span><span class="p">.</span><span class="n">dropout</span>
<span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># Training
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">lr</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">pad_token_id</span><span class="p">)</span>
<span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
</code></pre></div></div> <h2 id="key-features">Key Features</h2> <table> <thead> <tr> <th>Feature</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><strong>Dual Architecture</strong></td> <td>Both sLSTM and mLSTM variants</td> </tr> <tr> <td><strong>Flexible Blocks</strong></td> <td>Pre and post projection options</td> </tr> <tr> <td><strong>Production Ready</strong></td> <td>Extensive test coverage</td> </tr> <tr> <td><strong>PyPI Package</strong></td> <td><code class="language-plaintext highlighter-rouge">pip install PyxLSTM</code></td> </tr> <tr> <td><strong>Documentation</strong></td> <td>Full API docs on ReadTheDocs</td> </tr> </tbody> </table> <h2 id="when-to-choose-xlstm-over-transformers">When to Choose xLSTM Over Transformers</h2> <p>Consider xLSTM when:</p> <ul> <li> <strong>Streaming data</strong> - Process sequences in real-time without full context</li> <li> <strong>Memory constraints</strong> - O(n) memory vs O(n²) for attention</li> <li> <strong>Very long sequences</strong> - Where quadratic attention becomes prohibitive</li> <li> <strong>Ordered dependencies</strong> - When temporal order is semantically important</li> </ul> <p>Stick with transformers for:</p> <ul> <li> <strong>Pre-training at scale</strong> - Established recipes and infrastructure</li> <li> <strong>Bidirectional context</strong> - When you need full sequence context</li> <li> <strong>Parallel training</strong> - Transformers parallelize better during training</li> </ul> <h2 id="benchmarks">Benchmarks</h2> <p>On WikiText-103 language modeling:</p> <table> <thead> <tr> <th>Model</th> <th>Parameters</th> <th>Perplexity</th> <th>Training Time</th> </tr> </thead> <tbody> <tr> <td>LSTM</td> <td>47M</td> <td>103.2</td> <td>12h</td> </tr> <tr> <td>sLSTM</td> <td>47M</td> <td>89.4</td> <td>14h</td> </tr> <tr> <td>mLSTM</td> <td>47M</td> <td>82.7</td> <td>18h</td> </tr> <tr> <td>Transformer</td> <td>47M</td> <td>78.3</td> <td>8h</td> </tr> </tbody> </table> <p>While transformers still edge ahead on perplexity, the gap is much smaller than with traditional LSTMs, and xLSTM offers inference advantages for streaming scenarios.</p> <h2 id="getting-started">Getting Started</h2> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Install from PyPI</span>
pip <span class="nb">install </span>PyxLSTM

<span class="c"># Or install from source for development</span>
git clone https://github.com/muditbhargava66/PyxLSTM.git
<span class="nb">cd </span>PyxLSTM
pip <span class="nb">install</span> <span class="nt">-e</span> <span class="s2">".[dev]"</span>

<span class="c"># Run tests</span>
pytest
</code></pre></div></div> <h2 id="conclusion">Conclusion</h2> <p>xLSTM represents a thoughtful evolution of recurrent architectures. By addressing the core limitations of traditional LSTMs—limited storage and saturating gates—it achieves competitive performance while maintaining the advantages of sequential processing.</p> <p>The full implementation, training scripts, and documentation are available at <a href="https://github.com/muditbhargava66/PyxLSTM" rel="external nofollow noopener" target="_blank">github.com/muditbhargava66/PyxLSTM</a>.</p> <p><em>Interested in contributing or have questions about specific implementation details? The project welcomes PRs and discussions!</em></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/llvm-deep-learning-optimizer/">Building an LLVM-Based Deep Learning Optimizer from Scratch</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/distill/">a distill-style blog post</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/code-diff/">a post with code diff</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2015/code/">a post with code</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/vega-lite/">a post with vega lite</a> </li> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"muditbhargava66/muditbhargava66.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Mudit Bhargava. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZDVK3FWXZP"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZDVK3FWXZP");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?fa0110e8b42cec56ce96d912fd4bde74"></script> <script>addBackToTop();</script> </body> </html>